{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFZRpLpK02pB"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deSnZmVy02pF"
   },
   "source": [
    "\n",
    "# Deep Neural Networks\n",
    "\n",
    "## Session 04a : lecture\n",
    "### Neural Network with :\n",
    "- One hidden layer \n",
    "- ${Tanh}$ activation function\n",
    "- Compare with Tensorflow implementation\n",
    "\n",
    "\n",
    "<img src='../../images/prasami_color_tutorials_small.png' width='400' alt=\"By Pramod Sharma : pramod.sharma@prasami.com\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rt0pfyeA02pK"
   },
   "outputs": [],
   "source": [
    "# Lets import some libraries\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__T9cczP02pL"
   },
   "outputs": [],
   "source": [
    "# Some basic parameters\n",
    "inpDir = '../input'\n",
    "outDir = '../output'\n",
    "\n",
    "RANDOM_STATE = 24\n",
    "\n",
    "np.random.seed(RANDOM_STATE) # Set Random Seed for reproducible  results\n",
    "\n",
    "EPOCHS = 20000 # number of epochs\n",
    "ALPHA = 0.1 # learning rate\n",
    "NUM_SAMPLES = 1280 # How many samples we want to generate \n",
    "NOISE = 0.2 # Noise to be introduced in the data\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# parameters for Matplotlib\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 8),\n",
    "          'axes.labelsize': 'x-large',\n",
    "          'axes.titlesize':'x-large',\n",
    "          'xtick.labelsize':'x-large',\n",
    "          'ytick.labelsize':'x-large'\n",
    "         }\n",
    "\n",
    "CMAP = 'brg' # plt.cm.Spectral\n",
    "\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot history\n",
    "\n",
    "def fn_plot_hist(hist_df):\n",
    "    \n",
    "    '''\n",
    "    Args:\n",
    "        hist_df: a dataframe with following ccolumns\n",
    "            column 0: loss\n",
    "            column 1: accuracy\n",
    "            column 2: val_loss\n",
    "            column 3: val_accuracy\n",
    "            While plotting columns are accessed by index \n",
    "            so that even if the column names are different it will not throw exceptions.\n",
    "    '''\n",
    "    \n",
    "    # create figure and axis array\n",
    "    fig, axes = plt.subplots(1,2 , figsize = (15,6)) \n",
    "\n",
    "    \n",
    "    # properties  matplotlib.patch.Patch \n",
    "    props = dict(boxstyle='round', facecolor='aqua', alpha=0.4)\n",
    "\n",
    "    \n",
    "    # take first axis\n",
    "    ax = axes[0]\n",
    "    \n",
    "    \n",
    "    # Plot Column 0 and 2 (Loss and validation loss)\n",
    "    hist_df.plot(y = [hist_df.columns[0],hist_df.columns[2]], \n",
    "                 ax = ax,\n",
    "                 colormap=CMAP) \n",
    "\n",
    "    # get minimum values for plotting\n",
    "    lossmin = hist_df[hist_df.columns[0]].min()\n",
    "    \n",
    "    testmin = hist_df[hist_df.columns[2]].min()\n",
    "\n",
    "    \n",
    "    # little beautification\n",
    "    txtstr = \"Min {}: \\n Training : {:7.4f}\\n Testing   : {:7.4f}\".format(hist_df.columns[0],\n",
    "                                                                          lossmin,\n",
    "                                                                          testmin) #text to plot\n",
    "\n",
    "    # place a text box in upper middle in axes coords\n",
    "    ax.text(0.4, 0.95, txtstr,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=14,\n",
    "            verticalalignment='top',\n",
    "            bbox=props)\n",
    "\n",
    "    # x axis label\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "\n",
    "    # y axis lable\n",
    "    ax.set_ylabel(hist_df.columns[0].capitalize())\n",
    "    \n",
    "    # Show grids on the axis\n",
    "    ax.grid()\n",
    "    \n",
    "    \n",
    "    # take second axis object\n",
    "    ax = axes[1]\n",
    "\n",
    "    # plot column 1 and 3 (accuracy and validation accuracy)\n",
    "    hist_df.plot( y = [hist_df.columns[1], hist_df.columns[3]],\n",
    "                 ax = ax, \n",
    "                 colormap=CMAP)\n",
    "\n",
    "    \n",
    "    # little beautification\n",
    "    accmin = hist_df[hist_df.columns[1]].max()\n",
    "    \n",
    "    testmin = hist_df[hist_df.columns[3]].max()\n",
    "    \n",
    "    txtstr = \"Max {}: \\n Training : {:7.4f}\\n Testing   : {:7.4f}\".format(hist_df.columns[1],\n",
    "                                                                          accmin,\n",
    "                                                                          testmin) #text to plot\n",
    "    \n",
    "    # place a text box in upper left in axes coords\n",
    "    ax.text(0.4, 0.5, txtstr, \n",
    "            transform=ax.transAxes,\n",
    "            fontsize=14,\n",
    "            verticalalignment='top',\n",
    "            bbox=props)\n",
    "\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(hist_df.columns[1].capitalize())\n",
    "    ax.grid();\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bhVN20X02pN"
   },
   "source": [
    "## Generate Data Set\n",
    "<p style=\"font-family: Arial; font-size:1.1em;color:blue;\">\n",
    "Use Sklearn's dataset generator <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html\">make_moon</a>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXZ37P4G02pP"
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=NUM_SAMPLES, shuffle=True, noise=NOISE, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "PWMWQKQ702pQ",
    "outputId": "80ccd743-533a-4a5d-c009-9318021035a1"
   },
   "outputs": [],
   "source": [
    "# Lets Plot the data\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, marker='*', cmap=CMAP)\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_risr0-y02pR"
   },
   "outputs": [],
   "source": [
    "def fn_plot_decision_boundary(pred_func, X, y):\n",
    "    '''\n",
    "        Attrib:\n",
    "           pred_func : function based on predict method of the classifier\n",
    "           X : feature matrix\n",
    "           y : targets\n",
    "       Return:\n",
    "           None\n",
    "    '''\n",
    "    \n",
    "    # Set min and max values and give it some padding\n",
    "    xMin, xMax = X[:, 0].min() - .05, X[:, 0].max() + .05\n",
    "    yMin, yMax = X[:, 1].min() - .05, X[:, 1].max() + .05\n",
    "    \n",
    "    # grid size for mesh grid\n",
    "    h = 0.01\n",
    "    \n",
    "    # Generate a grid of points with distance 'h' between them\n",
    "    xx, yy = np.meshgrid(np.arange(xMin, xMax, h), np.arange(yMin, yMax, h))\n",
    "    \n",
    "    # Predict the function value for the whole grid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Make its shape same as that of xx \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Now we have Z value corresponding to each of the combination of xx and yy\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=CMAP, alpha = 0.6)\n",
    "    \n",
    "    # plot the points as well\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=CMAP, edgecolors='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ScVfbKXN02pT",
    "outputId": "b357d89b-6dbf-49f4-847b-174fcee65ef3"
   },
   "outputs": [],
   "source": [
    "#  Split the data in training and test sets to measure performance of the model.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE )\n",
    "\n",
    "print (X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you really want to save on space, convert to float32\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hf0cYC_L02pU"
   },
   "source": [
    "## Single neuron\n",
    "<img src=\"images/dnn_nb_s03_fig1.png\" width='350' align = 'left'>\n",
    "<img src=\"images/dnn_nb_s03_fig2.png\" width='550' align = 'right'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2az57rOj02pV"
   },
   "source": [
    "### For single perceptron:\n",
    "   \n",
    "$\n",
    "\\begin{aligned}\n",
    "a &=  \\sigma(z)\\\\\n",
    "a &=  \\sigma(x_1.w_1 + x_2.w_2 + b)\\\\ \n",
    "a &= \\sigma\\ ( [ x_1, x_2 ] \\circ\n",
    "\\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}  + b )\\\\\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ou0D2GhM02pW"
   },
   "source": [
    "#### For multiple Rows of X:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "a &= \\sigma\\ (\\begin{bmatrix} x_1^{(1)} & x_2^{(1)}\\\\ \n",
    "x_1^{(2)} & x_2^{(2)}\\\\\n",
    "x_1^{(...)} & x_2^{(...)}\\\\\n",
    "x_1^{(m)} & x_2^{(m)} \\end{bmatrix} \\circ\n",
    "\\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}  + b )\\\\\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtNpqiai02pW"
   },
   "source": [
    "In matrix form it can be represented as:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "a &= \\sigma\\ ( X_{shape = (m,2)} \\circ W_{shape = (2,1)}^{[1]} + b_{shape = (1,1)})\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "**Note:** Please note that Python is going to broadcast b in all $'m'$ rows. Avoid any confusion, always maintain dimensions of $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7w9j7Yo02pX"
   },
   "source": [
    "## Neural Network\n",
    "\n",
    "Let's start with simple network. Our data has **two** features. Hence size of input layer will also be two. The output is binary, we can code it as single column as well as double column output. The hidden layer could be of **any size**. One need to execute a handful of iterations to arrive at right size of hidden layer. For purpose of today's discussions, size of hidden layer is taken as shown below.\n",
    "<img src='images/dnn_nb_s04_fig1.png' width = '500'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TV2hxN-702pX"
   },
   "source": [
    "We will be using $\\tanh$ function for layer 1 (hidden layer) as it fits in majority of cases and its derivative can simply be represented as 1 -$\\tanh^2(z_1)$. Since our output is binary, it makes sense to use $\\text{Sigmoid}$ in the last layer.\n",
    "\n",
    "<img src='images/dnn_nb_s04_fig2.png' width = '500'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAFVIDSa02pX"
   },
   "source": [
    "## Forward Propagation\n",
    "\n",
    "### For single Neuron:\n",
    "$\n",
    "\\begin{aligned}\n",
    "a &= \\text{activation function} ( X \\circ W_1 + b)\\\\\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnyzmhlK02pY"
   },
   "source": [
    "Hence for hidden layer, we can write as follows:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "z_1^{[1]} & = X . W_1^{[1]} + b_1^{[1]}\\\\\n",
    "a_1^{[1]} & = \\tanh(z_1^{[1]}) \\\\\n",
    "\\\\\n",
    "z_2^{[1]} & = X . W_2^{[1]} + b_2^{[1]} \\\\\n",
    "a_2^{[1]} & = \\tanh(z_2^{[1]}) \\\\\n",
    "\\\\\n",
    "z_3^{[1]} & = X . W_3^{[1]} + b_3^{[1]} \\\\\n",
    "a_3^{[1]} & = \\tanh(z_3^{[1]}) \\\\\n",
    "\\\\\n",
    "z_4^{[1]} & = X . W_4^{[1]} + b_4^{[1]} \\\\\n",
    "a_4^{[1]} & = \\tanh(z_4^{[1]}) \\\\\n",
    "\\\\\n",
    "\\text{Or}\\\\\n",
    "a^{[1]} &= \\tanh(X \\circ \\begin{bmatrix} W_1^{[1]}, &W_2^{[1]}, &W_3^{[1]}, &W_4^{[1]}\\end{bmatrix} + b^{[1]} )\\\\\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "If we convert above to matrix version, we can say.\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "z_{shape = (m,4)}^{[1]} & = X_{shape = (m,2)} \\circ W_{shape=(2,4)}^{[1]} + b_{shape = (1,4)}^{[1]} \\\\\n",
    "\\\\\n",
    "a_{shape = (m,4)}^{[1]} & = \\tanh(z^{[1]}) \\\\\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Similarly for second layer.\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "z_{shape = (m, 1)}^{[2]} & = a_{shape = (m,4)}^{[1]} \\circ W_{shape=(4,1)}^{[2]} + b_{shape = (1,1)}^{[2]} \\\\\n",
    "\\\\\n",
    "a_{shape = (m, 1)}^{[2]} & = \\hat{y} = \\mathrm{sigmoid}(z^{[2]})\\\\\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Where:\n",
    "\n",
    "Sigmoid $\\sigma$: \n",
    "\n",
    "$\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$\n",
    "\n",
    "## Activation Functions\n",
    "\n",
    "### Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5prCl_I02pZ"
   },
   "outputs": [],
   "source": [
    "def fn_sigmoid(z):\n",
    "    '''\n",
    "    Args:\n",
    "        z : a matrix of z values of shape (m, n_output)\n",
    "    returns:\n",
    "        sigmoid values of z\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return 1 / ( 1 + np.exp ( -z ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iB-FU1yD02pa",
    "outputId": "d9bc602a-09be-4642-f0a9-ded12a831850"
   },
   "outputs": [],
   "source": [
    "sm = fn_sigmoid(np.asarray([-1, 0., 1.]))\n",
    "print (sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Oz4xPX002pa",
    "outputId": "742d81f4-b6af-4ee8-fa71-1a9275f6fbbf"
   },
   "outputs": [],
   "source": [
    "sm = fn_sigmoid(np.asarray([-np.inf, 0., np.inf]))\n",
    "print (sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exufhbJE02pb"
   },
   "outputs": [],
   "source": [
    "def fn_tanh(x):\n",
    "\n",
    "    return np.tanh(x)\n",
    "\n",
    "def fn_tanh_prime(x):\n",
    "\n",
    "    return 1.0 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = fn_tanh(np.asarray([-np.inf, 0., np.inf]))\n",
    "print (sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJP0FpaG02pc"
   },
   "source": [
    "### Is ourActivation Function working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "myOGW19S02pd",
    "outputId": "0cc6fc85-c49c-4ba2-9b75-5b477f1bd7ed"
   },
   "outputs": [],
   "source": [
    "np.tanh(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42hGCgjP02pd",
    "outputId": "47c658ae-d22e-4f3d-b1c5-b39aa806e8dd"
   },
   "outputs": [],
   "source": [
    "(1 - np.power(np.tanh(0.5), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9lUHZ8d02pe",
    "outputId": "fbe7d8f6-1736-4f34-a147-f661075d9dd7"
   },
   "outputs": [],
   "source": [
    "fn_tanh_prime(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0LlxhWX02pe"
   },
   "source": [
    "## Loss Function\n",
    "\n",
    "We need to minimize the error by adjusting ($Ws, bs$). We call the function that measures our error the <b>loss function</b>. A common choice with the sigmoid output is the cross-entropy loss. The loss for predictions $\\hat{y}$ with respect to the true labels $y$ is given by:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "L(\\hat{y_i}, y_i) =  -[y_i.log\\hat{y_i} + (1 - y_i) . log(1-\\hat{y_i})]\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "For all samples:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "J(\\hat{y}, y) =  -\\frac{1}{m}\\sum_{i=1}^{m}[y_i.log\\hat{y}_i + (1-y_i) . log(1-\\hat{y}_i)]\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "### In case of Binary Classification:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "L(\\hat{y_i}, y_i) =  -y_i.log\\hat{y_i}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "For all samples:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "J(\\hat{y}, y) =  -\\frac{1}{m}\\sum_{i=1}^{m}y_i.log\\hat{y}_i\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "\n",
    "We can use gradient descent to find its minimum. For purpose of this Session, let's use it in its simplest form - <b>batch gradient descent with fixed learning rate</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyCFgmBx02pf"
   },
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YcH2EYDw02pf"
   },
   "outputs": [],
   "source": [
    "###--------------------\n",
    "### Forward propagation\n",
    "###--------------------\n",
    "\n",
    "# function to evaluate the total loss on the dataset\n",
    "\n",
    "def calculate_loss(model, X, y):\n",
    "    '''\n",
    "    Args:\n",
    "        model: dictionay object containing weights and biases\n",
    "        X: Feature Matrix\n",
    "        y: Labels array\n",
    "    Returns:\n",
    "        Average loss\n",
    "    '''\n",
    "    \n",
    "\n",
    "    return 1./num_examples * data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ts1K_8ry02pg"
   },
   "source": [
    "## Predict Function\n",
    "\n",
    "For predictions, we will simply be using the forward propagation. No need to iterate or calculate the back propagation for supervised learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKgB1fF002pg"
   },
   "outputs": [],
   "source": [
    "# Helper function to predict an output (0 or 1)\n",
    "\n",
    "def predict(model, X):\n",
    "    '''\n",
    "     Args:\n",
    "         model\n",
    "         X: input features\n",
    "    Returns:\n",
    "        Predictions against the instances\n",
    "         \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    return a2>=0.5 # pick with one with highest probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjbFZqbG02ph"
   },
   "source": [
    "## For a single row of data x,\n",
    "\n",
    "<img src='images/dnn_nb_s04_fig3.png' style='width: 800px;'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "445K-wOF02ph"
   },
   "source": [
    "As an input, gradient descent needs the gradients (vector of derivatives) of the loss function with respect to our parameters: \n",
    "\n",
    "$\\frac{\\partial{L}}{\\partial{W_1}}(= \\partial{W^{[1]}})$, $\\frac{\\partial{L}}{\\partial{b_1}}(= \\partial{b^{[1]}})$, $\\frac{\\partial{L}}{\\partial{W_2}}(= \\partial{W^{[2]}})$, $\\frac{\\partial{L}}{\\partial{b_2}}(= \\partial{b^{[2]}})$. \n",
    "\n",
    "To calculate these gradients we use the <b>back-propagation algorithm</b>.\n",
    "\n",
    "**Note:** Loss is a function of $a^{[2]}$ which is a function of $z^{[2]}$ and so on. It can be further represented as follows:<br>\n",
    "$\n",
    "\\begin{aligned}\n",
    "Loss &= f_1(a^{[2]})\\\\\n",
    "a^{[2]} &= f_2(z^{[2]})\\\\\n",
    "z^{[2]} &= f_3(a^{[1]})\\\\\n",
    "a^{[1]} &= f_4(z^{[1]})\\\\\n",
    "\\text{Therefore:}\\\\\n",
    "\\frac{\\partial{Loss}}{\\partial{z^{[1]}}} &= \\frac{\\partial{Loss}}{\\partial{z^{[2]}}} \\circ\n",
    "\\frac{\\partial{z^{[2]}}}{\\partial{a^{[1]}}} \\circ \\frac{\\partial{a^{[1]}}}{\\partial{z^{[1]}}} \n",
    "\\end{aligned}\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbKpIxSd02pi"
   },
   "source": [
    "## Back-propagation for all Rows\n",
    "For all rows, equations will remain same and the values will be divided by <b><i>'m'</i></b>; number of samples.\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\partial{z^{[2]}}  & = a^{[2]} - y  \\\\\n",
    "\\partial{W^{[2]}}  & = \\frac{1}{m} a^{[1]T}\\circ \\partial{z^{[2]}} \\\\\n",
    "\\partial{b^{[2]}}  & = \\frac{1}{m} \\mathrm{np.sum}(\\partial{z^{[2]}}, axis = 0, keepdims = True) \\\\\n",
    "\\\\\n",
    "\\partial{z^{[1]}}  & = \\partial{z^{[2]}}\\circ  W^{[2]T} * ( 1-a^{[1]}**2)\\\\\n",
    "\\partial{W^{[1]}}  & = \\frac{1}{m} X^{T}\\circ \\partial{z^{[1]}} \\\\\n",
    "\\partial{b^{[1]}}  & = \\frac{1}{m} \\mathrm{np.sum}(\\partial{z^{[1]}}, axis = 0, keepdims = True) \\\\\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AO0dREm202pj"
   },
   "source": [
    "## Notes:\n",
    "\n",
    "\n",
    "We have transposed a few matrices in above calculations such as $a^{[1]}$, $W^{[2]}$ and X. A review of shapes of matrices will reveal that this adjustment is needed to have consistent sizes. e.g.\n",
    "\n",
    "- Shape of $a^{[1]}$ and $\\partial{z}^{[2]}$ are ( m, 4) and ( m, 1 ) respectively. Expected shape of $\\partial{W^{[2]}}$ is ( 4, 1 ) which is same as that of $W^{[2]}$.\n",
    "- In equation $\\partial{z^{[1]}}  = \\partial{z^{[2]}}\\circ  W^{[2]T} * ( 1-a^{[1]}**2)$ shape of $z^{[2]}$,  $W^{[2]}$ and $a^{[1]}$ are (m,1), (4,1) and (m,4). For element wise multiplication, expected shape of dot product of is $z^{[2]}$ and $W^{[2]}$ is ( m, 4 ).\n",
    "- Lastly, shape of $\\partial{W^{[1]}}$ is (2,4) and that of X and $\\partial{z^{[1]}}$ are ( m, 2 ) and ( m, 4 )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylYPQygT02pj"
   },
   "outputs": [],
   "source": [
    "# prepare the Model\n",
    "\n",
    "def build_model(nn_hdim, X, y, \n",
    "                epochs = EPOCHS, \n",
    "                alpha = ALPHA,\n",
    "                print_loss=False):\n",
    "    \n",
    "    '''\n",
    "    Args:\n",
    "        nn_hdim : Number of nodes in the hidden layer\n",
    "        X : Training features to train\n",
    "        y : Trainig targets (labels)\n",
    "        epochs : Number of passes through the training data for gradient descent\n",
    "        alpha : learning rate\n",
    "        print_loss : If True, print the loss every nnn iterations\n",
    "        \n",
    "    Returns:\n",
    "        Model: Dictionary object containing weights and biases\n",
    "    '''\n",
    "    \n",
    "    # Initialize the parameters to random values. We need to learn these.\n",
    "    # Weights\n",
    "    \n",
    "    '''\n",
    "    ##### \n",
    "            Change from Rand to Randn gives different shape fo the loss curve\n",
    "            Demonstrate in the class\n",
    "    #####\n",
    "    '''\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LRT_Uql02pk"
   },
   "outputs": [],
   "source": [
    "num_examples = len(X_train) # training set size\n",
    "nn_input_dim = 2 # input layer dimensionality\n",
    "nn_output_dim = 1 # output layer dimensionality\n",
    "\n",
    "# lists to facilitate plotting \n",
    "loss_hist = {}\n",
    "\n",
    "####################################\n",
    "### Gradient descent parameters  ###\n",
    "####################################\n",
    "# Try following values of alpha to see its effect on the graph\n",
    "#[ 0.0001, 0.001, 0.1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqXRbCxq02pl",
    "outputId": "1b4f29bf-5cbb-455b-82b0-5c2ee8d69561"
   },
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1,1)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWA64gB802pl",
    "outputId": "872231a9-fafd-4dc1-a71a-7e149630ab06"
   },
   "outputs": [],
   "source": [
    "# Build a model with a 4-dimensional hidden layer\n",
    "model = build_model(4, X_train, y_train,\n",
    "                    epochs = EPOCHS, \n",
    "                    alpha = ALPHA, \n",
    "                    print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySgcvDEA02pm",
    "outputId": "ebd9e235-1c7a-4692-9b8a-4355d3eee050"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YedWA8O02pn"
   },
   "source": [
    "### Would contest that we should have used higher epochs as loss is still coming down? How many epochs are sufficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(model, X_train)\n",
    "print('Accuracy score on Train Data :', accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MJhVHaoe17Kx",
    "outputId": "f54ec5b6-ad6b-4537-97ac-b3eae4326364"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALSrXj1y02pp",
    "outputId": "85d712a1-a35e-4f4f-9075-10c6013b8b48"
   },
   "outputs": [],
   "source": [
    "y_pred = predict(model, X_test)\n",
    "\n",
    "print('Accuracy score on Test Data :', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2-9J2srK02pp",
    "outputId": "d53e6cdf-9108-48f1-a812-8a8d32529912"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "id": "oa8WtTI202pn",
    "outputId": "3f0ca548-7715-47c7-b4fd-fac7924bc060"
   },
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(loss_hist)\n",
    "\n",
    "fn_plot_decision_boundary(lambda x: predict(model, x), X_train, y_train) # plot decision boundary for this plot\n",
    "\n",
    "plt.title(\"Decision Boundary\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "Y5CC6P4F02pn",
    "outputId": "706ee179-8e88-4084-c0be-14c5872fa3e4"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2 , figsize = (15,6))\n",
    "\n",
    "l_range = 5000\n",
    "\n",
    "ax = axes[0]\n",
    "\n",
    "loss_df.plot(x = 'epoch', y = 'loss', ax = ax)\n",
    "loss = loss_df['loss'].values\n",
    "\n",
    "# little beautification\n",
    "txtstr = \"Errors: \\n  Start : {:7.4f}\\n   End : {:7.4f}\".format(loss[0],loss[-1]) #text to plot\n",
    "# properties  matplotlib.patch.Patch \n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "# place a text box in upper left in axes coords\n",
    "\n",
    "ax.text(0.6, 0.95, txtstr, transform=ax.transAxes, fontsize=14,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_title('Overall')\n",
    "ax.grid();\n",
    "\n",
    "ax = axes[1]\n",
    "\n",
    "loss_df[-l_range:].plot(x = 'epoch', y = 'loss', ax = ax)\n",
    "\n",
    "# little beautification\n",
    "txtstr = \"Errors: \\n  Start : {:7.4f}\\n   End : {:7.4f}\".format(loss[-l_range],loss[-1]) #text to plot\n",
    "# properties  matplotlib.patch.Patch \n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "# place a text box in upper left in axes coords\n",
    "\n",
    "ax.text(0.6, 0.95, txtstr, transform=ax.transAxes, fontsize=14,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_title('Last {} records'.format(l_range))\n",
    "ax.grid();\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm  = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=[0,1])\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (4,4))\n",
    "\n",
    "disp.plot(ax = ax, cmap = 'Blues', colorbar=False)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wStSrcy-02pr"
   },
   "source": [
    "## Verifying using TensorFlow libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGOFjvtY02pr"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "try:\n",
    "    \n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    \n",
    "except:\n",
    "    \n",
    "    \n",
    "    print ('Invalid device or cannot modify virtual devices once initialized.')\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGOFjvtY02pr"
   },
   "outputs": [],
   "source": [
    "krnl_init = tf.keras.initializers.GlorotUniform(\n",
    "                                   seed=RANDOM_STATE\n",
    "                                   )\n",
    "\n",
    "# Define sequential model with same size and activation functions\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(4, activation='tanh', kernel_initializer=krnl_init),\n",
    "  tf.keras.layers.Dense(2, kernel_initializer=krnl_init)\n",
    "])\n",
    "\n",
    "# Using cross entropy for loss calculations\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RX-6Cngc02ps"
   },
   "source": [
    "[Optimizer that implements the Adam algorithm.](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\n",
    "\n",
    ">tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    name='Adam',\n",
    "    **kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUoVaXG102ps"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGZX5f7j02pt",
    "outputId": "30b8363c-123f-48b8-e042-0248c84bfb31",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit and evaluate\n",
    "\n",
    "history = model.fit(X_train, y_train ,\n",
    "                    validation_data = [X_test, y_test],\n",
    "                    epochs = 1000, verbose =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hyGJGjPC02pt",
    "outputId": "6947d3be-316f-4fc0-d367-797911acb945"
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test,  y_test, verbose = 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "meOZTCjb02pu"
   },
   "outputs": [],
   "source": [
    "# for probabilities add last layer as Softmax layer\n",
    "probability_model = tf.keras.Sequential([model,\n",
    "                                         tf.keras.layers.Softmax()\n",
    "                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_XQzn7A02pu",
    "outputId": "044dea8d-77b1-40b2-e905-09a4768d8727"
   },
   "outputs": [],
   "source": [
    "y_pred = probability_model(X_test).numpy().argmax(axis = 1)\n",
    "print('Accuracy score on Test Data :', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(history.history)\n",
    "\n",
    "fn_plot_hist(res_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pNTN6go02pv"
   },
   "source": [
    "# Assignment: S04-01\n",
    "-\tIs this model more accurate compared to previous model?\n",
    "-\tPrepare table by changing number of neurons in hidden layer, learning rate and observe change in results. Also comment on your results.\n",
    "\n",
    "**Note**: For faster execution of iterations you may want to comment-out calculations using `TensorFlow` libraries.\n",
    "\n",
    "|#|Dimension of hidden layer|Learning rate|Training Accuracy|Test Accuracy|Comment|\n",
    "|:-:|:-:|:-:|:-:|:-:|:--|\n",
    "|1|4|0.1|0.97|0.96|Base case||1|4|0.1|0.97|0.96|Base case|\n",
    "|2|1|1|???|???|???|\n",
    "|...|...|...|...|...|...|\n",
    "|n|...|...|...|...|...|\n",
    "\n",
    "-\tWhat lines will you change to convert it into multi-class prediction model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "S05a_one_hidden_layer_with_tanh_wip.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
