{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFZRpLpK02pB"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deSnZmVy02pF"
   },
   "source": [
    "\n",
    "# Deep Neural Networks\n",
    "\n",
    "## Session 08b - Lecture\n",
    "\n",
    "## Neural Network with ${Swish}$ activation function\n",
    "- Single hidden layer\n",
    "- **${Swish}$** activation function\n",
    "- Multi-class output\n",
    "- FIFA dataset \n",
    "\n",
    "<img src='../../images/prasami_color_tutorials_small.png' width='400' alt=\"By Pramod Sharma : pramod.sharma@prasami.com\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rt0pfyeA02pK"
   },
   "outputs": [],
   "source": [
    "# Lets import some libraries\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__T9cczP02pL"
   },
   "outputs": [],
   "source": [
    "# Some basic parameters\n",
    "inpDir = '../input'\n",
    "outDir = '../output'\n",
    "modelDir = '../models'\n",
    "subDir = 'fifa_2019'\n",
    "\n",
    "RANDOM_STATE = 24\n",
    "\n",
    "np.random.seed(RANDOM_STATE) # Set Random Seed for reproducible  results\n",
    "\n",
    "EPOCHS = 40001 # number of epochs\n",
    "ALPHA = 0.1 # learning rate\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "SAVE_MODEL = True # for saving the model\n",
    "\n",
    "# parameters for Matplotlib\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 10),\n",
    "          'axes.labelsize': 'x-large',\n",
    "          'axes.titlesize':'x-large',\n",
    "          'xtick.labelsize':'x-large',\n",
    "          'ytick.labelsize':'x-large'\n",
    "         }\n",
    "\n",
    "CMAP = plt.cm.Dark2\n",
    "\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bhVN20X02pN"
   },
   "source": [
    "## Read FIFA 2019 data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXZ37P4G02pP"
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(os.path.join(inpDir, 'fifa_2019.csv'))\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "PWMWQKQ702pQ",
    "outputId": "80ccd743-533a-4a5d-c009-9318021035a1"
   },
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_risr0-y02pR"
   },
   "outputs": [],
   "source": [
    "# cols = data_df.columns\n",
    "\n",
    "# for col in cols:\n",
    "#    print('\\nColumn Name ', col, ':', data_df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing rows with position = null\n",
    "data_df = data_df[data_df[\"Position\"].notnull()]\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following columns appear to be relevant for our analysis\n",
    "rel_cols = [\"Position\", 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling',\n",
    "            'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration',\n",
    "            'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower',\n",
    "            'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression',\n",
    "            'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure',\n",
    "            'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling',\n",
    "            'GKKicking', 'GKPositioning', 'GKReflexes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[rel_cols]\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there NaNs Still?\n",
    "\n",
    "data_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goalkeeper = 'GK'\n",
    "forward = ['ST', 'LW', 'RW', 'LF', 'RF', 'RS','LS', 'CF']\n",
    "midfielder = ['CM','RCM','LCM', 'CDM','RDM','LDM', 'CAM', 'LAM', 'RAM', 'RM', 'LM']\n",
    "defender = ['CB', 'RCB', 'LCB', 'LWB', 'RWB', 'LB', 'RB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign labels to goalkeepers\n",
    "data_df.loc[data_df[\"Position\"] == \"GK\", \"Position\"] = 0\n",
    "\n",
    "#Defenders\n",
    "data_df.loc[data_df[\"Position\"].isin(defender), \"Position\"] = 1\n",
    "\n",
    "#Midfielders\n",
    "data_df.loc[data_df[\"Position\"].isin(midfielder), \"Position\"] = 2\n",
    "\n",
    "#Forward\n",
    "data_df.loc[data_df[\"Position\"].isin(forward), \"Position\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Column \"Position\" to numeric so that Pandas does not complain\n",
    "data_df['Position'] = pd.to_numeric(data_df['Position'], downcast=\"integer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0: 'Goal Keeper', 1: 'Defender', 2: 'Mid-Fielder', 3: 'Forward'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_df[\"Position\"].values\n",
    "\n",
    "X = data_df.drop(\"Position\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "y = pd.get_dummies(y).values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ScVfbKXN02pT",
    "outputId": "b357d89b-6dbf-49f4-847b-174fcee65ef3"
   },
   "outputs": [],
   "source": [
    "#  Split the data in training and test sets to measure performance of the model.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, \n",
    "                                                    # shuffle=True, # This is True by Default\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=RANDOM_STATE )\n",
    "\n",
    "print (X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation and Softmax\n",
    "\n",
    "Softmax code is for demo purpose only. In general softmax functions can be highly unstable.\n",
    "If you must implement softmax, use libraries such as scipy.special.softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5prCl_I02pZ"
   },
   "outputs": [],
   "source": [
    "def fn_softmax(z):\n",
    "    \n",
    "    '''\n",
    "    Args:\n",
    "        z : a matrix of z values of shape (m, n_output)\n",
    "    returns:\n",
    "        Softmax values of z\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    exp_scores = np.exp(z)\n",
    "\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exufhbJE02pb"
   },
   "outputs": [],
   "source": [
    "def fn_actv(z):\n",
    "    '''\n",
    "    Args:\n",
    "        z: array z value\n",
    "    Returns:\n",
    "        a: array - activations\n",
    "    '''\n",
    "    return np.tanh(z)\n",
    "\n",
    "def fn_actv_prime(z):\n",
    "    '''\n",
    "    Args:\n",
    "        z: array\n",
    "    Returns:\n",
    "        a: array - activation prime\n",
    "    '''\n",
    "        \n",
    "    return 1.0 - np.tanh(z)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyCFgmBx02pf"
   },
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YcH2EYDw02pf"
   },
   "outputs": [],
   "source": [
    "# Helper function to evaluate the total loss on the dataset\n",
    "\n",
    "def calculate_loss(model, X_l, y_l):\n",
    "    '''\n",
    "    Args:\n",
    "        model: dictionay object containing weights and biases\n",
    "        X_l: Feature Matrix\n",
    "        y_l: Labels matrix (One-hot encoding expected)\n",
    "    Returns:\n",
    "        Average loss\n",
    "    '''\n",
    "    \n",
    "    # Extract Weights and biases from the model\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    m = X_l.shape[0] # ----------------------------------------  plotting testing loss as well\n",
    "    \n",
    "    # Layer 1\n",
    "    z1 = X_l.dot(W1) + b1  # Aggregation\n",
    "    a1 = fn_actv(z1)     # Activation\n",
    "\n",
    "    # Layer 2\n",
    "    z2 = a1.dot(W2) + b2 # Aggregation\n",
    "    a2 = fn_softmax(z2) # Activation\n",
    "    \n",
    "    # Calculating the loss\n",
    "    data_loss = -(y_l * np.log(a2) + (1 - y_l) * np.log(1 - a2) ).sum() \n",
    "\n",
    "    return 1./m * data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ts1K_8ry02pg"
   },
   "source": [
    "## Predict Function\n",
    "\n",
    "For predictions, we will simply be using the forward propagation. No need to iterate or calculate the back propagation for supervised learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKgB1fF002pg"
   },
   "outputs": [],
   "source": [
    "# Helper function to predict an output\n",
    "\n",
    "def predict(model, X_p):\n",
    "    '''\n",
    "     Args:\n",
    "         model: dict object containing weights and biases\n",
    "         X_p: Feature Matrix\n",
    "    Returns:\n",
    "        Predictions against the instances\n",
    "         \n",
    "    '''\n",
    "    # Extract Weights and biases from the model\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']  \n",
    "    \n",
    "    #***********************************\n",
    "    # Layer 1\n",
    "    z1 = X_p.dot(W1) + b1  # Aggregation\n",
    "    a1 = fn_actv(z1)     # Activation\n",
    "\n",
    "    # Layer 2\n",
    "    z2 = a1.dot(W2) + b2 # Aggregation\n",
    "    a2 = fn_softmax(z2)     # Activation\n",
    "\n",
    "   \n",
    "    return np.argmax(a2, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** - following function will load model if exists, otherwise train from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylYPQygT02pj"
   },
   "outputs": [],
   "source": [
    "# Prepare the Model\n",
    "\n",
    "def build_model(nn_hdim, \n",
    "                X : np.ndarray, y : np.ndarray,\n",
    "                X_t: np.ndarray, y_t: np.ndarray,\n",
    "                alpha = 0.1,\n",
    "                epochs = 20000, \n",
    "                print_loss=False):\n",
    "    \n",
    "    '''\n",
    "    Args:\n",
    "        nn_hdim : number of nodes in hidden layer\n",
    "        X   : Training features to train\n",
    "        y   : Trainig targets (labels)\n",
    "        X_t : Testing features to train\n",
    "        y_t : Testing targets (labels)\n",
    "        alpha : learning rate\n",
    "        epochs : Number of passes through the training data for gradient descent\n",
    "        print_loss : If True, print the loss every nnn iterations\n",
    "        \n",
    "    Returns:\n",
    "        Model: Dictionary object containing weights and biases\n",
    "    '''\n",
    "    if os.path.isfile(modelFilePath):\n",
    "        print (f'Pre-trained model found at {modelFilePath}. Loading....')\n",
    "        \n",
    "        with open(modelFilePath, 'rb') as file:\n",
    "            model_details = pickle.load(file)\n",
    "            model = model_details['weights']\n",
    "        print (f'Loaded existing model with\\n {model_details[\"description\"]}\\n\\n')\n",
    "        print ('#'*50)\n",
    "        W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "        \n",
    "    else:\n",
    "        print (f'No model found at {modelFilePath}. Initializing....')\n",
    "\n",
    "        W1 = np.random.rand(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)\n",
    "        b1 = np.zeros((1, nn_hdim))\n",
    "        \n",
    "        W2 = np.random.rand(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)\n",
    "        b2 = np.zeros((1, nn_output_dim))\n",
    "    \n",
    "        model ={}\n",
    "    \n",
    "    startTime = time()\n",
    "    \n",
    "    curr_loss = 0\n",
    "    epoch, loss, t_loss, acc, t_acc = [], [], [], [], []\n",
    "    num_examples = X.shape[0] # training set size\n",
    "    \n",
    "    for i in range(0, epochs):\n",
    "        \n",
    "        epochTime = time()\n",
    "        \n",
    "        #### Forward ------------------------------\n",
    "        # Layer 1\n",
    "        z1 = X.dot(W1) + b1 \n",
    "        a1 = fn_actv(z1)    # activation\n",
    "        \n",
    "        # Layer 2\n",
    "        z2 = a1.dot(W2) + b2 \n",
    "        a2 = fn_softmax(z2)    # activation\n",
    "        \n",
    "        \n",
    "        ### Back Prop ------------------------------\n",
    "        #Layer 2\n",
    "        dz2 = a2- y\n",
    "       \n",
    "        dW2 = (a1.T).dot(dz2)  \n",
    "        assert(W2.shape == dW2.shape), 'Shape of W2 {} and dW2 {} do not match'.format(W2.shape, dW2.shape)\n",
    "        \n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "        assert(b2.shape == db2.shape), 'Shape of b2 {} and db2 {} do not match'.format(b2.shape, db2.shape)\n",
    "        \n",
    "        da1= dz2.dot(W2.T)\n",
    "        \n",
    "        #Layer 1\n",
    "        dz1 = da1 * fn_actv_prime(a1)\n",
    "        dW1 = (X.T).dot(dz1)\n",
    "        assert(W1.shape == dW1.shape), 'Shape of W1 {} and dW1 {} do not match'.format(W1.shape, dW1.shape)\n",
    "        \n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "        assert(b1.shape == db1.shape), 'Shape of b1 {} and db1 {} do not match'.format(b1.shape, db1.shape)\n",
    "        \n",
    "        \n",
    "        # Weights update\n",
    "        W1 += -alpha * dW1 /num_examples\n",
    "        b1 += -alpha * db1 /num_examples\n",
    "        \n",
    "        W2 += -alpha * dW2 /num_examples\n",
    "        b2 += -alpha * db2 /num_examples\n",
    "        \n",
    "        \n",
    "        model = {'W1': W1, 'b1': b1,\n",
    "                 'W2': W2, 'b2': b2}\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            \n",
    "            # Keep count of epoch\n",
    "            epoch.append(i)\n",
    "            \n",
    "            # calculate loss\n",
    "            curr_loss = calculate_loss (model, X, y)\n",
    "            loss.append(curr_loss)\n",
    "            \n",
    "            ct_loss = calculate_loss ( model, X_t, y_t)\n",
    "            t_loss.append(ct_loss)\n",
    "            \n",
    "            # calculate accuracy\n",
    "            y_pred = predict(model, X)\n",
    "            curr_acc = accuracy_score(np.argmax(y, axis = 1), y_pred )\n",
    "            acc.append(curr_acc)\n",
    "            \n",
    "            yt_pred = predict(model, X_t)\n",
    "            currt_acc = accuracy_score(np.argmax(y_t, axis = 1), yt_pred )\n",
    "            t_acc.append(currt_acc)\n",
    "            \n",
    "        # Print the loss.\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            print(f'Epoch # :{i}')\n",
    "            print('Time of execution till now: {:0.2f} mins'.format( (time() - startTime ) / 60.) )\n",
    "            print('Last epoch: {:0.2f} ms'.format ( (time() - epochTime) * 1000 ) )\n",
    "            print('Train: Loss: %f  ---- Accuracy: %f' %(curr_loss, curr_acc))\n",
    "            print('Test:  Loss: %f  ---- Accuracy: %f' %(ct_loss, currt_acc))\n",
    "            print(\"-\" *50)\n",
    "    \n",
    "    loss_hist['epoch'] = epoch\n",
    "    loss_hist['loss'] = loss\n",
    "    loss_hist['test_loss'] = t_loss\n",
    "    loss_hist['acc'] = acc\n",
    "    loss_hist['test_acc'] = t_acc\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LRT_Uql02pk"
   },
   "outputs": [],
   "source": [
    "nn_input_dim = X_train.shape[1] # input layer dimensionality\n",
    "nn_output_dim = y_train.shape[1] # output layer dimensionality\n",
    "\n",
    "nn_hidden_dim = 18\n",
    "\n",
    "alpha = ALPHA\n",
    "\n",
    "epochs = EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWA64gB802pl",
    "outputId": "872231a9-fafd-4dc1-a71a-7e149630ab06"
   },
   "outputs": [],
   "source": [
    "loss_hist = {}\n",
    "\n",
    "modelFilePath = os.path.join(modelDir,subDir, 'S08_fifa_1L_swish_model.pkl')\n",
    "\n",
    "model = build_model(nn_hidden_dim, X_train, y_train, X_test, y_test, alpha, epochs, print_loss = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(loss_hist)\n",
    "loss_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_details = {}\n",
    "\n",
    "model_details['description'] = f'''Hidden layers : 1;\n",
    "Layer sizes: \n",
    "    Input-{nn_input_dim}; Hidden-{nn_hidden_dim},Output-{nn_output_dim}\n",
    "Activation Function :  Swish'''\n",
    "\n",
    "model_details['weights'] = model\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    \n",
    "    with open(modelFilePath, 'wb') as file:\n",
    "        pickle.dump(model_details, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    loss_df.to_csv(os.path.join(modelDir, subDir, 'S07_fifa_1L_model_loss.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySgcvDEA02pm",
    "outputId": "ebd9e235-1c7a-4692-9b8a-4355d3eee050"
   },
   "outputs": [],
   "source": [
    "print ('Model Weights:\\n', model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2 , figsize = (15,6),)\n",
    "\n",
    "l_range = 5 # ignoring first few records\n",
    "x_var = 'epoch'\n",
    "y1_var = 'loss'\n",
    "y2_var = 'test_loss'\n",
    "\n",
    "ax = axes[0]\n",
    "\n",
    "loss_df[l_range:].plot(x = x_var, y = [y1_var, y2_var], ax = ax)\n",
    "\n",
    "# get values for plotting\n",
    "st_loss = loss_df[y1_var].iloc[0]\n",
    "end_loss = loss_df[y1_var].iloc[-1]\n",
    "t_st_loss = loss_df[y2_var].iloc[0]\n",
    "t_end_loss = loss_df[y2_var].iloc[-1]\n",
    "\n",
    "# little beautification\n",
    "train_txtstr = \"Train Loss: \\n  Start : {:7.4f}\\n    End : {:7.4f}\".format( st_loss, end_loss )    # Train text to plot\n",
    "\n",
    "test_txtstr = \"Test Loss: \\n  Start : {:7.4f}\\n    End : {:7.4f}\".format( t_st_loss, t_end_loss ) # Test text to plot\n",
    "txtstr = ' {} \\n{}'.format(train_txtstr, test_txtstr)\n",
    "# properties  matplotlib.patch.Patch \n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "# place a text box in upper left in axes coords\n",
    "\n",
    "ax.text(0.4, 0.95, txtstr, transform=ax.transAxes, fontsize=12,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "ax.set_xlabel(x_var)\n",
    "ax.set_ylabel(y1_var)\n",
    "ax.set_title('Overall')\n",
    "ax.grid();\n",
    "\n",
    "ax = axes[1]\n",
    "\n",
    "y1_var = 'acc'\n",
    "y2_var = 'test_acc'\n",
    "\n",
    "loss_df[l_range:].plot(x = x_var, y = [y1_var, y2_var], ax = ax)\n",
    "\n",
    "# get values for plotting\n",
    "st_loss = loss_df[y1_var].iloc[0]\n",
    "end_loss = loss_df[y1_var].iloc[-1]\n",
    "t_st_loss = loss_df[y2_var].iloc[0]\n",
    "t_end_loss = loss_df[y2_var].iloc[-1]\n",
    "\n",
    "# little beautification\n",
    "train_txtstr = \"Train Accuracy: \\n  Start : {:7.4f}\\n    End : {:7.4f}\".format( st_loss, end_loss )    # Train text to plot\n",
    "\n",
    "test_txtstr = \"Test Accuracy: \\n  Start : {:7.4f}\\n    End : {:7.4f}\".format( t_st_loss, t_end_loss ) # Test text to plot# properties  matplotlib.patch.Patch \n",
    "txtstr = ' {} \\n{}'.format(train_txtstr, test_txtstr)\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "# place a text box in upper left in axes coords\n",
    "\n",
    "ax.text(0.4, 0.95, txtstr, transform=ax.transAxes, fontsize=12,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "ax.set_xlabel(x_var)\n",
    "ax.set_ylabel(y1_var)\n",
    "ax.set_title('Accuracies'.format(l_range))\n",
    "ax.grid();\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TpTJ41pP02po",
    "outputId": "f3bda5d5-1d08-4307-ca44-ff1354e9d676"
   },
   "outputs": [],
   "source": [
    "y_pred = predict(model, X_train)\n",
    "print('Accuracy score on Train Data :{:.5f}'.format( accuracy_score(np.argmax(y_train, axis = 1), y_pred ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MJhVHaoe17Kx",
    "outputId": "f54ec5b6-ad6b-4537-97ac-b3eae4326364"
   },
   "outputs": [],
   "source": [
    "print(classification_report(np.argmax(y_train, axis = 1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALSrXj1y02pp",
    "outputId": "85d712a1-a35e-4f4f-9075-10c6013b8b48"
   },
   "outputs": [],
   "source": [
    "y_pred = predict(model, X_test)\n",
    "\n",
    "print('Accuracy score on Test Data :{:.5f}'.format(accuracy_score(np.argmax(y_test, axis = 1), y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2-9J2srK02pp",
    "outputId": "d53e6cdf-9108-48f1-a812-8a8d32529912"
   },
   "outputs": [],
   "source": [
    "print(classification_report(np.argmax(y_test, axis = 1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_erJ4AZz02pw"
   },
   "outputs": [],
   "source": [
    "cm  = confusion_matrix(np.argmax(y_test, axis = 1), y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=labels.values())\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (4,4))\n",
    "\n",
    "disp.plot(ax = ax, cmap = 'Blues', xticks_rotation = 'vertical', colorbar=False)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df[loss_df['test_loss'] == loss_df['test_loss'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "S05a_one_hidden_layer_with_tanh_wip.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
