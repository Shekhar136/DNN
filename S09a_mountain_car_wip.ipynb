{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19fa7d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b30282",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "## Session 09a\n",
    "\n",
    "## Mountain Car with DQN\n",
    "\n",
    "<img src='../../images/prasami_color_tutorials_small.png' style = 'width:400px;' alt=\"By Pramod Sharma : pramod.sharma@prasami.com\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7005920f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-24 08:51:10.431441: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-24 08:51:11.224872: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/pks/RL/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2022-12-24 08:51:11.224924: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/pks/RL/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2022-12-24 08:51:11.224930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Import statements\n",
    "\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "905d16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic parameters\n",
    "inpDir = '../input'\n",
    "outDir = '../output'\n",
    "modelDir = '../models'\n",
    "\n",
    "# parameters for Matplotlib\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (12, 9),\n",
    "          'axes.labelsize': 'x-large',\n",
    "          'axes.titlesize':'x-large',\n",
    "          'xtick.labelsize':'x-large',\n",
    "          'ytick.labelsize':'x-large'\n",
    "         }\n",
    "\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "RANDOM_STATE = 24\n",
    "\n",
    "STEPS = 400\n",
    "\n",
    "NUM_ITER = 201\n",
    "\n",
    "BATCH = 32\n",
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "ALPHA = 0.001\n",
    "\n",
    "EPSILON = 1\n",
    "\n",
    "EPSILON_DECAY = 0.05\n",
    "\n",
    "EPSILON_MIN = 0.01\n",
    "\n",
    "MAX_LENGHT = 20000\n",
    "\n",
    "ENV_NAME = 'MountainCar-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72c1dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClearMemory(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        #print ('clearing at end of {}.'.format(epoch))\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60f7164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCarTrain:\n",
    "    \n",
    "    #Initialize Class Variables\n",
    "    def __init__(self,env):\n",
    "        \n",
    "        self.env = env\n",
    "        self.gamma = GAMMA\n",
    "\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = EPSILON_DECAY\n",
    "\n",
    "        self.epsilon_min = EPSILON_MIN\n",
    "\n",
    "\n",
    "        self.learingRate = ALPHA\n",
    "\n",
    "        self.replayBuffer = deque(maxlen=MAX_LENGHT)\n",
    "        \n",
    "        self.trainNetwork = self.createNetwork()\n",
    "\n",
    "        self.episodeNum = STEPS\n",
    "\n",
    "        self.iterationNum = NUM_ITER\n",
    "\n",
    "        self.numPickFromBuffer = BATCH\n",
    "\n",
    "        self.targetNetwork = self.createNetwork()\n",
    "\n",
    "        self.targetNetwork.set_weights(self.trainNetwork.get_weights())\n",
    "        \n",
    "        self.modelPath = os.path.join(modelDir,'mountain_car_models', 'trainNetworkInEPS{}.h5')\n",
    "\n",
    "        \n",
    "    # Create a Sequential Model \n",
    "    def createNetwork(self):\n",
    "        \n",
    "        model = tf.keras.models.Sequential()\n",
    "        \n",
    "        state_shape = self.env.observation_space.shape\n",
    "\n",
    "        model.add ( tf.keras.layers.Dense(24, \n",
    "                                          activation='relu',\n",
    "                                          input_shape=state_shape ) )\n",
    "        \n",
    "        model.add(tf.keras.layers.Dense(48,\n",
    "                                        activation='relu') )\n",
    "        \n",
    "        model.add(tf.keras.layers.Dense(self.env.action_space.n,\n",
    "                                        activation='linear'))\n",
    "\n",
    "        model.compile(loss='mse', \n",
    "                      optimizer=tf.keras.optimizers.Adam(lr=self.learingRate) )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # Epsilon Greedy Policy\n",
    "    def getBestAction(self,state):\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "\n",
    "        if np.random.rand(1) < self.epsilon:\n",
    "            \n",
    "            action = np.random.randint(0, self.env.action_space.n) # Random Action \n",
    "        \n",
    "        else:\n",
    "            action=np.argmax(self.trainNetwork.predict(state, verbose = 0)[0]) # Best action\n",
    "\n",
    "        return action\n",
    "\n",
    "    \n",
    "    # Training the model from the buffer we have saved\n",
    "    def trainFromBuffer(self):\n",
    "        \n",
    "        if len(self.replayBuffer) < self.numPickFromBuffer:\n",
    "            return\n",
    "        \n",
    "        samples = random.sample(self.replayBuffer,self.numPickFromBuffer)\n",
    "        \n",
    "        npSamples = np.array(samples, dtype=object)\n",
    "        \n",
    "        statesTemp, actionsTemp, rewardsTmp, newStatesTemp, donesTmp = np.hsplit(npSamples, 5)\n",
    "        \n",
    "        states = np.concatenate((np.squeeze(statesTemp[:])), axis = 0)\n",
    "        \n",
    "        rewards = rewardsTmp.reshape(self.numPickFromBuffer,).astype(float)\n",
    "        \n",
    "        targets = self.trainNetwork.predict(states, verbose = 0)\n",
    "        \n",
    "        newStates = np.concatenate(np.concatenate(newStatesTemp))\n",
    "        \n",
    "        dones = np.concatenate(donesTmp).astype(bool)\n",
    "        \n",
    "        notdones = ~dones\n",
    "        \n",
    "        notdones = notdones.astype(float)\n",
    "        \n",
    "        dones = dones.astype(float)\n",
    "        \n",
    "        Q_futures = self.targetNetwork.predict(newStates, verbose = 0).max(axis = 1)\n",
    "        \n",
    "        targets[(np.arange(self.numPickFromBuffer), actionsTemp.reshape(self.numPickFromBuffer,).astype(int))] = rewards * dones + (rewards + Q_futures * self.gamma)*notdones\n",
    "        \n",
    "        self.trainNetwork.fit(states, targets, epochs=1, verbose=0, callbacks=ClearMemory())\n",
    "        \n",
    "        \n",
    "    def orginalTry(self, currentState, eps):\n",
    "        \n",
    "        rewardSum = 0\n",
    "        \n",
    "        max_position=-99\n",
    "\n",
    "        for i in range(self.iterationNum):\n",
    "            \n",
    "            bestAction = self.getBestAction(currentState)\n",
    "\n",
    "            #show the animation every 50 eps\n",
    "            #if eps%50==0:\n",
    "            #    #env.render()\n",
    "            #    #pass\n",
    "\n",
    "            newState, reward, terminate, truncate, info = env.step(bestAction)\n",
    "            \n",
    "            done = terminate or truncate\n",
    "\n",
    "            newState = newState.reshape(1, 2)\n",
    "\n",
    "            # Keep track of max position\n",
    "            if newState[0][0] > max_position:\n",
    "                \n",
    "                max_position = newState[0][0]\n",
    "\n",
    "\n",
    "            # Adjust reward for task completion\n",
    "            if newState[0][0] >= 0.5:\n",
    "                reward += 10\n",
    "\n",
    "            self.replayBuffer.append([currentState, bestAction, reward, newState, done])\n",
    "\n",
    "            self.trainFromBuffer()\n",
    "\n",
    "            rewardSum += reward\n",
    "\n",
    "            currentState = newState\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if i >= 199:\n",
    "            \n",
    "            print(\"Failed to finish task in epsoide {}\".format(eps))\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"Success in epsoide {}, used {} iterations!\".format(eps, i))\n",
    "            \n",
    "            self.trainNetwork.save( self.modelPath.format(eps) )\n",
    "\n",
    "        #Sync\n",
    "        self.targetNetwork.set_weights(self.trainNetwork.get_weights())\n",
    "\n",
    "        print(\"now epsilon is {}, the reward is {} maxPosition is {}\".format(max(self.epsilon_min, self.epsilon), rewardSum,max_position))\n",
    "        self.epsilon -= self.epsilon_decay\n",
    "\n",
    "    def start(self):\n",
    "        \n",
    "        for eps in range(self.episodeNum):\n",
    "            \n",
    "            currentState=env.reset()[0].reshape(1,2)\n",
    "            \n",
    "            self.orginalTry(currentState, eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49b033d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb4921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-24 08:51:12.088566: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-24 08:51:12.107747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-24 08:51:12.107968: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-24 08:51:12.108915: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-24 08:51:12.109670: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-24 08:51:12.109836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-24 08:51:12.109972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-24 08:51:12.556062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-24 08:51:12.556272: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-24 08:51:12.556420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-24 08:51:12.556558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4112 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "/home/pks/RL/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "2022-12-24 08:51:13.542952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-12-24 08:51:14.019626: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x55a02adb52c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-12-24 08:51:14.019662: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2022-12-24 08:51:14.023480: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-12-24 08:51:14.086098: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-12-24 08:51:14.129890: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to finish task in epsoide 0\n",
      "now epsilon is 1, the reward is -200.0 maxPosition is -0.42732512950897217\n",
      "Failed to finish task in epsoide 1\n",
      "now epsilon is 0.95, the reward is -200.0 maxPosition is -0.39987611770629883\n",
      "Failed to finish task in epsoide 2\n",
      "now epsilon is 0.8999999999999999, the reward is -200.0 maxPosition is -0.3053654432296753\n",
      "Failed to finish task in epsoide 3\n",
      "now epsilon is 0.8499999999999999, the reward is -200.0 maxPosition is -0.3684562146663666\n",
      "Failed to finish task in epsoide 4\n",
      "now epsilon is 0.7999999999999998, the reward is -200.0 maxPosition is -0.42298054695129395\n",
      "Failed to finish task in epsoide 5\n",
      "now epsilon is 0.7499999999999998, the reward is -200.0 maxPosition is -0.30854588747024536\n",
      "Failed to finish task in epsoide 6\n",
      "now epsilon is 0.6999999999999997, the reward is -200.0 maxPosition is -0.24878241121768951\n",
      "Failed to finish task in epsoide 7\n",
      "now epsilon is 0.6499999999999997, the reward is -200.0 maxPosition is -0.3384263515472412\n",
      "Failed to finish task in epsoide 8\n",
      "now epsilon is 0.5999999999999996, the reward is -200.0 maxPosition is -0.3422296941280365\n",
      "Failed to finish task in epsoide 9\n",
      "now epsilon is 0.5499999999999996, the reward is -200.0 maxPosition is -0.1469389796257019\n",
      "Failed to finish task in epsoide 10\n",
      "now epsilon is 0.4999999999999996, the reward is -200.0 maxPosition is -0.24126680195331573\n",
      "Failed to finish task in epsoide 11\n",
      "now epsilon is 0.4499999999999996, the reward is -200.0 maxPosition is -0.4663807153701782\n",
      "Failed to finish task in epsoide 12\n",
      "now epsilon is 0.39999999999999963, the reward is -200.0 maxPosition is -0.3693538308143616\n",
      "Failed to finish task in epsoide 13\n",
      "now epsilon is 0.34999999999999964, the reward is -200.0 maxPosition is -0.2947729229927063\n",
      "Failed to finish task in epsoide 14\n",
      "now epsilon is 0.29999999999999966, the reward is -200.0 maxPosition is -0.07436999678611755\n",
      "Failed to finish task in epsoide 15\n",
      "now epsilon is 0.24999999999999967, the reward is -200.0 maxPosition is -0.29251715540885925\n",
      "Failed to finish task in epsoide 16\n",
      "now epsilon is 0.19999999999999968, the reward is -200.0 maxPosition is -0.21291573345661163\n",
      "Failed to finish task in epsoide 17\n",
      "now epsilon is 0.1499999999999997, the reward is -200.0 maxPosition is -0.053420621901750565\n",
      "Failed to finish task in epsoide 18\n",
      "now epsilon is 0.09999999999999969, the reward is -200.0 maxPosition is -0.3526856303215027\n",
      "Failed to finish task in epsoide 19\n",
      "now epsilon is 0.049999999999999684, the reward is -200.0 maxPosition is 0.26551586389541626\n",
      "Failed to finish task in epsoide 20\n",
      "now epsilon is 0.01, the reward is -200.0 maxPosition is -0.1563553661108017\n"
     ]
    }
   ],
   "source": [
    "dqn=MountainCarTrain(env=env)\n",
    "dqn.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4a23d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
